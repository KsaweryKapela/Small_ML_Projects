from typing import Callable, List, Optional, Tuple
import pandas as pd
from sklearn.base import TransformerMixin, BaseEstimator
import torch

class BertTransformer(BaseEstimator, TransformerMixin):
    def __init__(self, bert_tokenizer, bert_model, max_length):
        
        self.device = "cuda:0"
        self.bert_tokenizer = bert_tokenizer
        self.bert_model = bert_model
        self.bert_model.eval()
        self.max_length = max_length
        self.embedding_func = self.embedding_func = lambda x: x[0][:, 0, :].squeeze()

    def _tokenize(self, text: str) -> Tuple[torch.tensor, torch.tensor]:
        tokenized_text = self.bert_tokenizer.encode_plus(text,
                                                         add_special_tokens=True,
                                                         return_tensors='pt',
                                                         max_length=self.max_length,
                                                         )["input_ids"].to(self.device)

        attention_mask = [1] * len(tokenized_text)

        return (
            torch.tensor(tokenized_text).unsqueeze(0).to(self.device),
            torch.tensor(attention_mask).unsqueeze(0).to(self.device),
        )

    def _tokenize_and_predict(self, text: str) -> torch.tensor:
        
        tokenized, attention_mask = self._tokenize(text)

        embeddings = self.bert_model(tokenized[0], attention_mask)
        
        return self.embedding_func(embeddings)

    def transform(self, text: List[str]):
        if isinstance(text, pd.Series):
            text = text.tolist()

        with torch.no_grad():
            return torch.stack([self._tokenize_and_predict(string) for string in text]).cpu()
    
    def fit(self, X, y=None):
        return self